{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qlIMhr9POH5",
        "outputId": "98df40f2-b49e-4c4d-842b-201db8659e81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ja_ftP5OK4Cj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transform\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertConfig\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "\n",
        "def load_model(network, optimizer, PATH ):\n",
        "    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
        "    network.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(rf'Loaded model and optimizer from {PATH}')\n",
        "    else:\n",
        "        print(rf'Loaded model from {PATH}')\n",
        "\n",
        "    return\n",
        "\n",
        "def load_transformer_based_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForMaskedLM.from_pretrained(model_path ,output_hidden_states=True)\n",
        "    return tokenizer, model\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(BERT, self).__init__()\n",
        "        self.bert_model = model\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.out = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self,ids,mask,token_type_ids):\n",
        "        o2= self.bert_model(ids,mask,token_type_ids).hidden_states[0]\n",
        "        o2= o2.max(dim=1).values\n",
        "        o2 = self.dropout(o2)\n",
        "        # print('------->>>>>>>>>',o2.values.shape)\n",
        "        out= self.out(o2)\n",
        "\n",
        "        return out, o2\n",
        "\n",
        "class BertDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer,max_length):\n",
        "        super(BertDataset, self).__init__()\n",
        "\n",
        "        self.train_csv= data\n",
        "        self.tokenizer=tokenizer\n",
        "        self.max_length=max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_csv)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        text1 = self.train_csv[index]\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text1 ,\n",
        "            None,\n",
        "            pad_to_max_length=True,\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "            max_length=self.max_length,\n",
        "            truncation = True\n",
        "        )\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(token_type_ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long)\n",
        "\n",
        "\n",
        "def get_prediction(data, tokenizer, network):\n",
        "  dataset= BertDataset(data, tokenizer, max_length=200 )\n",
        "  data_loader=DataLoader(dataset=dataset, shuffle=False)\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  device = torch.device(device)\n",
        "  network = network.to(device)\n",
        "  network.eval()\n",
        "  l = []\n",
        "  emb = []\n",
        "  for batch in tqdm(data_loader):\n",
        "    input_ids, token_type_ids, attention_masks = batch\n",
        "    input_ids = input_ids.to(device)\n",
        "    token_type_ids = token_type_ids.to(device)\n",
        "    attention_masks = attention_masks.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds, hidden_states = network(input_ids, attention_masks,token_type_ids)\n",
        "        preds =  torch.where(preds >= 0, 1, 0)\n",
        "        l.append(preds)\n",
        "        emb.append(hidden_states)\n",
        "  return torch.vstack(l).cpu().numpy().tolist(), torch.vstack(emb).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"ka05ar/banglabert-sentiment\" #change here........ (for toxicity, use \"FredZhang7/one-for-all-toxicity-v3\", for sentiment use \"ka05ar/banglabert-sentiment\", for hatespeech use \"Hate-speech-CNERG/bengali-abusive-MuRIL\", for SarcaSm use \"raquiba/sarcasm-detection-BanglaSARC\")\n",
        "tokenizer, b_model = load_transformer_based_model(model_path)\n",
        "tokenizer.add_tokens(['<Name>' ,'<Gender>'])\n",
        "b_model.resize_token_embeddings(len(tokenizer))\n",
        "model=BERT(b_model)\n",
        "# load saved_model\n",
        "saved_model_path = r'/content/drive/MyDrive/#Research/# GB/saved_model/Senti_Original.pth' #change here........\n",
        "load_model(model, None, PATH = saved_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPHHnV1JLMAh",
        "outputId": "d1cedaf0-2426-4c95-ca10-a5545e82c7bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at ka05ar/banglabert-sentiment and are newly initialized: ['generator_lm_head.bias', 'generator_predictions.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator_predictions.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from /content/drive/MyDrive/#Research/# GB/saved_model/Senti_Original.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['অপুরভ ভাই অসাম', 'পোলা খোর কখন আসবো']  #change here... make sure to pass the sentences within a list\n",
        "label, embedding = get_prediction(sentences, tokenizer, model)\n",
        "print('\\nPredicted Labels:',label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um3EVSIKL8mT",
        "outputId": "a86fb21d-faaf-4327-a28c-91534049e307"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted Labels: [[1], [0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}